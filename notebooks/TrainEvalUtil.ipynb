{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainEvalUtil.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML9mAFHEpfKmsgFSuZZS+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veerendra12/CS598-DL4H-Project/blob/main/notebooks/TrainEvalUtil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as sklm\n",
        "\n",
        "import import_ipynb\n",
        "from Utils import get_device, memory_report\n",
        "from Configuration import CONFIG\n",
        "from Utils import save_checkpoint"
      ],
      "metadata": {
        "id": "IYWkFUc8f-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxFPC7vwfpxa"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, checkpoint_prefix):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    model: A CNN model\n",
        "    train_dataloader: the DataLoader of the training data\n",
        "    n_epoch: number of epochs to train\n",
        "    optimizer: optimizer for training\n",
        "    criterion: Loss function\n",
        "    Return:\n",
        "        model: trained model \n",
        "    \n",
        "    \"\"\"\n",
        "    DEVICE = get_device()\n",
        "    NUM_EPOCHS = CONFIG['NUM_EPOCHS']\n",
        "    # When resuming a partial training session\n",
        "    EPOCH_START = CONFIG['EPOCH_START']\n",
        "    BASE_DIR = CONFIG['BASE_DIR']\n",
        "\n",
        "    train_epoch_losses, validate_epoch_losses = [], [] \n",
        "    memories = {'cpu-ram-free': [],\n",
        "                'cuda-memory-allocated': [],\n",
        "                'cuda-memory-cached': [],\n",
        "                'gpu-0-mem-free': [],\n",
        "                'gpu-0-mem-total': [],\n",
        "                'gpu-0-mem-util': []\n",
        "                }    \n",
        "    train_times = []\n",
        "    validate_times = []\n",
        "\n",
        "\n",
        "    if CONFIG['RESUME_TRAINING']:\n",
        "      print(\"Loading earlier run summary data for resuming the training\")\n",
        "      prev_summary_df = pd.read_csv(CONFIG['BASE_DIR'] + 'results/summary.csv') \n",
        "\n",
        "      train_epoch_losses.append(prev_summary_df['Training Loss'])\n",
        "      validate_epoch_losses.append(prev_summary_df['Validation Loss'])\n",
        "\n",
        "      train_times.append(prev_summary_df['Training Time (mns)'])\n",
        "      validate_times.append(prev_summary_df['Validation Time (mns)'])\n",
        "\n",
        "      memories['cpu-ram-free'].append(prev_summary_df['cpu-ram-free'])\n",
        "      memories['cuda-memory-allocated'].append(prev_summary_df['cuda-memory-allocated'])\n",
        "      memories['cuda-memory-cached'].append(prev_summary_df['cuda-memory-cached'])\n",
        "      memories['gpu-0-mem-free'].append(prev_summary_df['gpu-0-mem-free'])\n",
        "      memories['gpu-0-mem-total'].append(prev_summary_df['gpu-0-mem-total'])\n",
        "      memories['gpu-0-mem-util'].append(prev_summary_df['gpu-0-mem-util'])\n",
        "    \n",
        "    for epoch in range(EPOCH_START, NUM_EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        train_row={}\n",
        "        train_curr_epoch_loss = 0\n",
        "        for data in train_dataloader:\n",
        "            inputs = data[0].to(DEVICE)\n",
        "            labels = data[1].to(DEVICE)            \n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(inputs)\n",
        "            y_hat = y_hat.to(DEVICE)\n",
        "            labels = labels.type(torch.FloatTensor)\n",
        "            labels = labels.to(DEVICE)\n",
        "            loss = criterion(y_hat, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_curr_epoch_loss += loss.cpu().data.numpy()\n",
        "\n",
        "        train_curr_epoch_loss = train_curr_epoch_loss / len(train_dataloader)\n",
        "        train_epoch_losses.append(train_curr_epoch_loss)\n",
        "        train_time = time.time() - epoch_start_time\n",
        "        train_times.append(round(train_time/60, 4))\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train curr_epoch_loss={train_curr_epoch_loss}\")\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(train_time // 60, train_time % 60))\n",
        "        \n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        validate_curr_epoch_loss = 0\n",
        "        for i, data in enumerate(val_dataloader):    \n",
        "            inputs = data[0].to(DEVICE)\n",
        "            labels = data[1].to(DEVICE)            \n",
        "            labels = labels.type(torch.FloatTensor)\n",
        "            labels = labels.to(DEVICE)\n",
        "            true_labels = labels.cpu().data.numpy()            \n",
        "            y_hat = model(inputs)\n",
        "            y_hat = y_hat.to(DEVICE)\n",
        "            probs = y_hat.cpu().data.numpy()                            \n",
        "            loss = criterion(y_hat, labels)\n",
        "\n",
        "            validate_curr_epoch_loss += loss.cpu().data.numpy()\n",
        "\n",
        "        validate_curr_epoch_loss = validate_curr_epoch_loss / len(val_dataloader)\n",
        "        validate_epoch_losses.append(validate_curr_epoch_loss)\n",
        "        validation_time = time.time() - epoch_start_time\n",
        "        validate_times.append(round(validation_time/60, 4))\n",
        "\n",
        "        print(f\"Epoch {epoch}: Validate curr_epoch_loss={validate_curr_epoch_loss}\")\n",
        "        print('Validation complete in {:.0f}m {:.0f}s'.format(validation_time // 60, validation_time % 60))\n",
        "        \n",
        "        memory = memory_report()\n",
        "        memories['cpu-ram-free'].append(memory['cpu-ram-free'])\n",
        "        memories['cuda-memory-allocated'].append(memory['cuda-memory-allocated'])\n",
        "        memories['cuda-memory-cached'].append(memory['cuda-memory-cached'])\n",
        "        memories['gpu-0-mem-free'].append(memory['gpu-0-mem-free'])\n",
        "        memories['gpu-0-mem-total'].append(memory['gpu-0-mem-total'])\n",
        "        memories['gpu-0-mem-util'].append(memory['gpu-0-mem-util'])    \n",
        "\n",
        "        results = pd.DataFrame({'Iteration': range(len(train_epoch_losses)), \n",
        "                                'Training Loss': train_epoch_losses,\n",
        "                                'Validation Loss': validate_epoch_losses,\n",
        "                                'Training Time (mns)': train_times,\n",
        "                                'Validation Time (mns)': validate_times,\n",
        "                                'cpu-ram-free': memories['cpu-ram-free'],\n",
        "                                'cuda-memory-allocated': memories['cuda-memory-allocated'],\n",
        "                                'cuda-memory-cached': memories['cuda-memory-cached'],\n",
        "                                'gpu-0-mem-free': memories['gpu-0-mem-free'],\n",
        "                                'gpu-0-mem-total': memories['gpu-0-mem-total'],\n",
        "                                'gpu-0-mem-util': memories['gpu-0-mem-util']})\n",
        "        results.to_csv(BASE_DIR + 'results/summary.csv')  \n",
        "\n",
        "        # save model\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\":optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint, BASE_DIR + \"results/\" + checkpoint_prefix + str(epoch) + \".pth\")          \n",
        "        \n",
        "    return model, results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, dataset, dataloader):\n",
        "    \"\"\"\n",
        "    Use the trained/best model and valuate on validation dataset\n",
        "    Args:\n",
        "    model: trained model\n",
        "    dataloader: validation dataloader\n",
        "    :return:\n",
        "        Y_pred: prediction of model on the dataloder.\n",
        "        Y_test: truth labels. \n",
        "    \"\"\"\n",
        "    DEVICE = get_device()\n",
        "    BATCH_SIZE = CONFIG['BATCH_SIZE']\n",
        "    CLASS_LABELS = CONFIG['CLASS_LABELS']\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    pred_df = pd.DataFrame(columns=[\"Image Index\"])\n",
        "    true_df = pd.DataFrame(columns=[\"Image Index\"])\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs = data[0].to(DEVICE)\n",
        "        labels = data[1].to(DEVICE)\n",
        "        true_labels = labels.cpu().data.numpy()\n",
        "        batch_size = true_labels.shape\n",
        "        y_hat = model(inputs)\n",
        "        probs = y_hat.cpu().data.numpy()\n",
        "\n",
        "        for j in range(0, batch_size[0]):\n",
        "            thisrow = {}\n",
        "            truerow = {}\n",
        "            thisrow[\"Image Index\"] = dataset.df.index[BATCH_SIZE * i + j]\n",
        "            truerow[\"Image Index\"] = dataset.df.index[BATCH_SIZE * i + j]\n",
        "\n",
        "\n",
        "            for k in range(len(CLASS_LABELS)):\n",
        "                thisrow[\"prob_\" + CLASS_LABELS[k]] = probs[j, k]\n",
        "                truerow[CLASS_LABELS[k]] = true_labels[j, k]\n",
        "\n",
        "            pred_df = pred_df.append(thisrow, ignore_index=True)\n",
        "            true_df = true_df.append(truerow, ignore_index=True)\n",
        "\n",
        "    auc_df = compute_auc(pred_df, true_df)\n",
        "\n",
        "    return pred_df, true_df, auc_df"
      ],
      "metadata": {
        "id": "rgdguyfrfwTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_auc(pred_df, true_df):\n",
        "  \"\"\"\n",
        "  Get Prediction and True values for Validation dataset and generate AUC Vaules for Validation dataset\n",
        "  \"\"\"\n",
        "  BASE_DIR = CONFIG['BASE_DIR']\n",
        "  CLASS_LABELS = CONFIG['CLASS_LABELS']\n",
        "  RUN_PREFIX = CONFIG['RUN_PREFIX']\n",
        "  auc_df = pd.DataFrame(columns=[\"label\", \"auc\"])\n",
        "\n",
        "  for column in true_df:\n",
        "\n",
        "      if column not in CLASS_LABELS:\n",
        "          continue\n",
        "      actual = true_df[column]\n",
        "      pred = pred_df[\"prob_\" + column]\n",
        "      thisrow = {}\n",
        "      thisrow['label'] = column\n",
        "      thisrow['auc'] = np.nan\n",
        "      try:\n",
        "          thisrow['auc'] = sklm.roc_auc_score(\n",
        "          actual.values.astype(int), pred.values)\n",
        "\n",
        "          \n",
        "      except BaseException as e:\n",
        "          print(\"can't calculate auc for \" + str(column))\n",
        "          print(e)\n",
        "      auc_df = auc_df.append(thisrow, ignore_index=True)\n",
        "\n",
        "\n",
        "  pred_df.to_csv(BASE_DIR + \"results/\" + RUN_PREFIX + \"_preds.csv\", index=False)\n",
        "  auc_df.to_csv(BASE_DIR + \"results/\" + RUN_PREFIX + \"_aucs.csv\", index=False)\n",
        "  true_df.to_csv(BASE_DIR + \"results/\" + RUN_PREFIX + \"_true.csv\", index=False)\n",
        "  print(auc_df)\n",
        "  return auc_df  "
      ],
      "metadata": {
        "id": "kO9VwLdZhKwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_epoch_loss(results):\n",
        "  loss_train = results['Training Loss']\n",
        "  loss_val = results['Validation Loss']\n",
        "  epochs = results['Iteration']\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "N71sfTzN2rcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}